---
doc_type: hypothesis-highlights
url: 'http://freecoder.com/2017-01-04/53197.html'
---
# 深度学习调参技巧
## Metadata
- Author: [freecoder.com]()
- Title: 深度学习调参技巧
- Reference: http://freecoder.com/2017-01-04/53197.html
- Category: #source/article🗞
- Tags:
## Highlights
- 将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。

- 画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。

- 如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。

- 如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。

- 建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。

- 如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。

- 如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。

   - Annotation: 学习率比较重要- 如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。

- 词向量embedding大小：128，256

- 训练时跟踪损失函数值，训练集和验证集准确率。

- 合理性检查，确定模型，数据和其他地方没有问题。

- 使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。

- 不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。

- 网络层数： 先从1层开始。

- 每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。

- learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。

- dropout： 0.5

- L2正则：1.0，超过10的很少见。

- 在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。

- 除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。

