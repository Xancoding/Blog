---
doc_type: hypothesis-highlights
url: 'https://tangshusen.me/Dive-into-DL-PyTorch/'
---
# 2.2 数据操作 - Dive-into-DL-PyTorch
## Metadata
- Author: [tangshusen.me]()
- Title: 2.2 数据操作 - Dive-into-DL-PyTorch
- Reference: https://tangshusen.me/Dive-into-DL-PyTorch/
- Category: #source/article🗞
- Tags:
## Highlights
- 通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。近年来，仰仗着大数据集和强大的硬件，深度学习已逐渐成为处理图像、文本语料和声音信号等复杂高维度数据的主要方法。

- 概率论、统计学和模式识别等工具帮助自然科学的实验学家们从数据回归到自然定律，从而发现了如欧姆定律（描述电阻两端电压和流经电阻电流关系的定律）这类可以用线性模型完美表达的一系列自然法则。

- 现代统计学在20世纪的真正起飞要归功于数据的收集和发布。


- Tags:

- 另一个对深度学习有重大影响的领域是神经科学与心理学。既然人类显然能够展现出智能，那么对于解释并逆向工程人类智能机理的探究也在情理之中。


- Tags:

- 时至今日，绝大多数神经网络都包含以下的核心原则。交替使用线性处理单元与非线性处理单元，它们经常被称为“层”。使用链式法则（即反向传播）来更新网络的参数。

- 由于数据和计算力的稀缺，从经验上来说，如核方法、决策树和概率图模型等统计工具更优。它们不像神经网络一样需要长时间的训练，并且在强大的理论保证下提供可以预测的结果。


- Tags:

- 互联网的崛起、价廉物美的传感器和低价的存储器令我们越来越容易获取大量数据。加之便宜的计算力，尤其是原本为电脑游戏设计的GPU的出现，上文描述的情况改变了许多。一瞬间，原本被认为不可能的算法和模型变得触手可及。

- 这也相应导致了机器学习和统计学的最优选择从广义线性模型及核方法变化为深度多层神经网络。这样的变化正是诸如多层感知机、卷积神经网络、长短期记忆循环神经网络和Q学习等深度学习的支柱模型在过去10年从坐了数十年的冷板凳上站起来被“重新发现”的原因。


- Tags:

- 优秀的容量控制方法，如丢弃法，使大型网络的训练不再受制于过拟合（大型神经网络学会记忆大部分训练数据的行为） [3]。这是靠在整个网络中注入噪声而达到的，如训练时随机将权重替换为随机的数字 [4]。

- 注意力机制解决了另一个困扰统计学超过一个世纪的问题：如何在不增加参数的情况下扩展一个系统的记忆容量和复杂度。


- Tags:

- 记忆网络 [6]和神经编码器—解释器 [7]这样的多阶设计使得针对推理过程的迭代建模方法变得可能。这些模型允许重复修改深度网络的内部状态，这样就能模拟出推理链条上的各个步骤，就好像处理器在计算过程中修改内存一样。

- 另一个重大发展是生成对抗网络的发明 [8]。传统上，用在概率分布估计和生成模型上的统计方法更多地关注于找寻正确的概率分布，以及正确的采样算法。生成对抗网络的关键创新在于将采样部分替换成了任意的含有可微分参数的算法。

- 许多情况下单个GPU已经不能满足在大型数据集上进行训练的需要。过去10年内我们构建分布式并行训练算法的能力已经有了极大的提升。设计可扩展算法的最大瓶颈在于深度学习优化算法的核心：随机梯度下降需要相对更小的批量。与此同时，更小的批量也会降低GPU的效率。如果使用1,024个GPU，每个GPU的批量大小为32个样本，那么单步训练的批量大小将是32,000个以上。近年来李沐 [11]、Yang You等人 [12]以及Xianyan Jia等人 [13]的工作将批量大小增至多达64,000个样例，并把在ImageNet数据集上训练ResNet-50模型的时间降到了7分钟。与之对比，最初的训练时间需要以天来计算。


- Tags:

- 并行计算的能力也为至少在可以采用模拟情况下的强化学习的发展贡献了力量。并行计算帮助计算机在围棋、雅达利游戏、星际争霸和物理模拟上达到了超过人类的水准。

- 系统研究者负责构建更好的工具，统计学家建立更好的模型。这样的分工使工作大大简化。举例来说，在2014年时，训练一个逻辑回归模型曾是卡内基梅隆大学布置给机器学习方向的新入学博士生的作业问题。时至今日，这个问题只需要少于10行的代码便可以完成，普通的程序员都可以做到。


- Tags:

- 虽然长期处于公众视野之外，但是机器学习已经渗透到了我们工作和生活的方方面面。直到近年来，在此前认为无法被解决的问题以及直接关系到消费者的问题上取得突破性进展后，机器学习才逐渐变成公众的焦点。这些进展基本归功于深度学习。


- Tags:

- 机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。


- Tags:

- 在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出，而本书要重点探讨的深度学习是具有多级表示的表征学习方法。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。

- 深度学习可以逐级表示越来越抽象的概念或模式。

- 最终，模型能够较容易根据更高级的表示完成给定的任务，如识别图像中的物体。值得一提的是，作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。


- Tags:

- 因此，深度学习的一个外在特点是端到端的训练。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。


- Tags:

- 除端到端的训练以外，我们也正在经历从含参数统计模型转向完全无参数的模型。当数据非常稀缺时，我们需要通过简化对现实的假设来得到实用的模型。当数据充足时，我们就可以用能更好地拟合现实的无参数模型来替代这些含参数模型。这也使我们可以得到更精确的模型，尽管需要牺牲一些可解释性。

- 相对其它经典的机器学习方法而言，深度学习的不同在于：对非最优解的包容、对非凸非线性优化的使用，以及勇于尝试没有被证明过的方法。这种在处理统计问题上的新经验主义吸引了大量人才的涌入，使得大量实际问题有了更好的解决方案。尽管大部分情况下需要为深度学习修改甚至重新发明已经存在数十年的工具，但是这绝对是一件非常有意义并令人兴奋的事。


- Tags:

- 机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。

- 作为机器学习的一类，表征学习关注如何自动找出表示数据的合适方式。

- 深度学习是具有多级表示的表征学习方法。它可以逐级表示越来越抽象的概念或模式。


- Tags:

- 所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view

- 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。

- 前面我们看到如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。


- Tags:

- 前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。


- Tags:

- 注：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。

- 我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！

- 还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。


- Tags:

- 所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。


- Tags:

- 如果不想要被继续追踪，可以调用.detach()将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用with torch.no_grad()将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。

- Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。

- 注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。

- 数学上，如果有一个函数值和自变量都为向量的函数 y⃗ =f(x⃗ )y⃗=f(x⃗) \vec{y}=f(\vec{x})<math><semantics><mrow><mover accent="true"><mi>y</mi><mo>⃗</mo></mover><mo>=</mo><mi>f</mi><mo>(</mo><mover accent="true"><mi>x</mi><mo>⃗</mo></mover><mo>)</mo></mrow><annotation encoding="application/x-tex">\vec{y}=f(\vec{x})</annotation></semantics></math>y​=f(x), 那么 y⃗ y⃗ \vec{y}<math><semantics><mrow><mover accent="true"><mi>y</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{y}</annotation></semantics></math>y​ 关于 x⃗ x⃗ \vec{x}<math><semantics><mrow><mover accent="true"><mi>x</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x}</annotation></semantics></math>x 的梯度就是一个雅可比矩阵（Jacobian matrix）

- 现在我们解释2.3.1节留下的问题，为什么在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor? 简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。

- 问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… 为了避免这个问题，我们不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量。

- 所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量，举个例子，假设y由自变量x计算而来，w是和y同形的张量，则y.backward(w)的含义是：先计算l = torch.sum(y * w)，则l是个标量，然后求l对自变量x的导数。

- 现在 z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量。

- 注意，x.grad是和x同形的张量。

- 上面提到，y2.requires_grad=False，所以不能调用 y2.backward()，会报错

- 此外，如果我们想要修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我么可以对tensor.data进行操作。

- x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播


- Tags:

- 线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题

- 顾名思义，线性回归假设输出与各个输入之间是线性关系

- 接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）

- 在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。

- 其中常数 1212 \frac 1 2<math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac 1 2</annotation></semantics></math>21​ 使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）。

- 通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量

- 在模型训练中，我们希望找出一组模型参数，记为 w∗1,w∗2,b∗w1∗,w2∗,b∗ w_1^*, w_2^*, b^*<math><semantics><mrow><msubsup><mi>w</mi><mn>1</mn><mo>∗</mo></msubsup><mo separator="true">,</mo><msubsup><mi>w</mi><mn>2</mn><mo>∗</mo></msubsup><mo separator="true">,</mo><msup><mi>b</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">w_1^*, w_2^*, b^*</annotation></semantics></math>w1∗​,w2∗​,b∗，来使训练样本平均损失最小

- 当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。

   - Annotation: 当模型比较复杂时，误差最小化的解无法直接通过公式求出来，这个时候就需要通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。

解析解和数值解- 在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。

- 在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）BB \mathcal{B}<math><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math>B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

- 在上式中，∣B∣∣B∣ |\mathcal{B}|<math><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">B</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\mathcal{B}|</annotation></semantics></math>∣B∣ 代表每个小批量中的样本个数（批量大小，batch size），ηη \eta<math><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math>η 称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。

- 模型训练完成后，我们将模型参数 w1,w2,bw1,w2,b w_1, w_2, b<math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">w_1, w_2, b</annotation></semantics></math>w1​,w2​,b 在优化算法停止时的值分别记作 wˆ1,wˆ2,bˆw^1,w^2,b^ \hat{w}_1, \hat{w}_2, \hat{b}<math><semantics><mrow><msub><mover accent="true"><mi>w</mi><mo>^</mo></mover><mn>1</mn></msub><mo separator="true">,</mo><msub><mover accent="true"><mi>w</mi><mo>^</mo></mover><mn>2</mn></msub><mo separator="true">,</mo><mover accent="true"><mi>b</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{w}_1, \hat{w}_2, \hat{b}</annotation></semantics></math>w^1​,w^2​,b^。注意，这里我们得到的并不一定是最小化损失函数的最优解 w∗1,w∗2,b∗w1∗,w2∗,b∗ w_1^*, w_2^*, b^*<math><semantics><mrow><msubsup><mi>w</mi><mn>1</mn><mo>∗</mo></msubsup><mo separator="true">,</mo><msubsup><mi>w</mi><mn>2</mn><mo>∗</mo></msubsup><mo separator="true">,</mo><msup><mi>b</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">w_1^*, w_2^*, b^*</annotation></semantics></math>w1∗​,w2∗​,b∗，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型 x1wˆ1+x2wˆ2+bˆx1w^1+x2w^2+b^ x_1 \hat{w}_1 + x_2 \hat{w}_2 + \hat{b}<math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><msub><mover accent="true"><mi>w</mi><mo>^</mo></mover><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mover accent="true"><mi>w</mi><mo>^</mo></mover><mn>2</mn></msub><mo>+</mo><mover accent="true"><mi>b</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">x_1 \hat{w}_1 + x_2 \hat{w}_2 + \hat{b}</annotation></semantics></math>x1​w^1​+x2​w^2​+b^ 来估算训练数据集以外任意一栋面积（平方米）为x1x1 x_1<math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math>x1​、房龄（年）为x2x2 x_2<math><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math>x2​的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。

- 在深度学习中，我们可以使用神经网络图直观地表现模型结构。为了更清晰地展示线性回归作为神经网络的结构，图3.1使用神经网络图表示本节中介绍的线性回归模型。神经网络图隐去了模型参数权重和偏差。

- 输入个数也叫特征数或特征向量维度。

- 由于输入层并不涉及计算，按照惯例，图3.1所示的神经网络的层数为1。所以，线性回归是一个单层神经网络。输出层中负责计算 oo o<math><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math>o 的单元又叫神经元。在线性回归中，oo o<math><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math>o 的计算依赖于 x1x1 x_1<math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math>x1​ 和 x2x2 x_2<math><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math>x2​。也就是说，输出层中的神经元和输入层中各个输入完全连接。因此，这里的输出层又叫全连接层（fully-connected layer）或稠密层（dense layer）。

- 在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。

- 结果很明显，后者比前者更省时。因此，我们应该尽可能采用矢量计算，以提升计算效率。

- 其中梯度是损失有关3个为标量的模型参数的偏导数组成的向量：

- 和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型、训练数据、损失函数和优化算法。

- 应该尽可能采用矢量计算，以提升计算效率。


- Tags:

- 尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，会导致我们很难深入理解深度学习是如何工作的。因此，本节将介绍如何只利用Tensor和autograd来实现一个线性回归的训练。

- 我们构造一个简单的人工训练数据集，它可以使我们能够直观比较学到的参数和真实的模型参数的区别。

- 我们将上面的plt作图函数以及use_svg_display函数和set_figsize函数定义在d2lzh_pytorch包里。以后在作图时，我们将直接调用d2lzh_pytorch.plt。由于plt在d2lzh_pytorch包中是一个全局变量，我们在作图前只需要调用d2lzh_pytorch.set_figsize()即可打印矢量图并设置图的尺寸。

- 这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。

- 在训练中，我们将多次迭代模型参数。在每次迭代中，我们根据当前读取的小批量数据样本（特征X和标签y），通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数。

- 回忆一下自动求梯度一节。由于变量l并不是一个标量，所以我们可以调用.sum()将其求和得到一个标量，再运行l.backward()得到该变量有关模型参数的梯度。注意在每次更新完参数后不要忘了将参数的梯度清零。

- 在一个迭代周期（epoch）中，我们将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数num_epochs和学习率lr都是超参数，分别设3和0.03。在实践中，大多超参数都需要通过反复试错来不断调节。虽然迭代周期数设得越大模型可能越有效，但是训练时间可能过长。而有关学习率对模型的影响，我们会在后面“优化算法”一章中详细介绍

- 在上一节从零开始的实现中，我们需要定义模型参数，并使用它们一步步描述模型是怎样计算的。当模型结构变得更复杂时，这些步骤将变得更繁琐。其实，PyTorch提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。

- 事实上我们还可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。

- 可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。

- 回顾图3.1中线性回归在神经网络图中的表示。作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层。

- 注意：torch.nn仅支持输入一个batch的样本不支持单个样本输入，如果只有单个样本，可使用input.unsqueeze(0)来添加一维。

- 我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。

- 注：如果这里的net是用3.3.3节一开始的代码自定义的，那么上面代码会报错，net[0].weight应改为net.linear.weight，bias亦然。因为net[0]这样根据下标访问子模块的写法只有当net是个ModuleList或者Sequential实例时才可以，详见4.1节。

- PyTorch在nn模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类。

- 同样，我们也无须自己实现小批量随机梯度下降算法。torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。

- 我们还可以为不同子网络设置不同的学习率，这在finetune时经常用到。

- 有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。

- 在使用Gluon训练模型时，我们通过调用optim实例的step函数来迭代模型参数。按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均。

- torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。

- 使用PyTorch可以更简洁地实现模型。

- 本节以softmax回归模型为例，介绍神经网络中的分类模型。

- 我们通常使用离散的数值来表示类别，例如y1=1,y2=2,y3=3y1=1,y2=2,y3=3 y_1=1, y_2=2, y_3=3<math><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>=</mo><mn>2</mn><mo separator="true">,</mo><msub><mi>y</mi><mn>3</mn></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">y_1=1, y_2=2, y_3=3</annotation></semantics></math>y1​=1,y2​=2,y3​=3。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。

- softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的ww w<math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math>w）、偏差包含3个标量（带下标的bb b<math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math>b），且对每个输入计算o1,o2,o3o1,o2,o3 o_1, o_2, o_3<math><semantics><mrow><msub><mi>o</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>o</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>o</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">o_1, o_2, o_3</annotation></semantics></math>o1​,o2​,o3​这3个输出

- softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出o1,o2,o3o1,o2,o3 o_1, o_2, o_3<math><semantics><mrow><msub><mi>o</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>o</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>o</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">o_1, o_2, o_3</annotation></semantics></math>o1​,o2​,o3​的计算都要依赖于所有的输入x1,x2,x3,x4x1,x2,x3,x4 x_1, x_2, x_3, x_4<math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, x_3, x_4</annotation></semantics></math>x1​,x2​,x3​,x4​，softmax回归的输出层也是一个全连接层。

- softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：

- 然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。

- 另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。


- Tags:

- 对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。

- 我们可以像线性回归那样使用平方损失函数∥yˆ(i)−y(i)∥2/2∥y^(i)−y(i)∥2/2 \|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}\|^2/2<math><semantics><mrow><mi mathvariant="normal">∥</mi><msup><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>−</mo><msup><mi mathvariant="bold-italic">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}\|^2/2</annotation></semantics></math>∥y^​(i)−y(i)∥2/2。然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果y(i)=3y(i)=3 y^{(i)}=3<math><semantics><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">y^{(i)}=3</annotation></semantics></math>y(i)=3，那么我们只需要yˆ(i)3y^3(i) \hat{y}^{(i)}_3<math><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>3</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^{(i)}_3</annotation></semantics></math>y^​3(i)​比其他两个预测值yˆ(i)1y^1(i) \hat{y}^{(i)}_1<math><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>1</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^{(i)}_1</annotation></semantics></math>y^​1(i)​和yˆ(i)2y^2(i) \hat{y}^{(i)}_2<math><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>2</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^{(i)}_2</annotation></semantics></math>y^​2(i)​大就行了。即使yˆ(i)3y^3(i) \hat{y}^{(i)}_3<math><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>3</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^{(i)}_3</annotation></semantics></math>y^​3(i)​值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如yˆ(i)1=yˆ(i)2=0.2y^1(i)=y^2(i)=0.2 \hat y^{(i)}_1=\hat y^{(i)}_2=0.2<math><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>1</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>2</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">\hat y^{(i)}_1=\hat y^{(i)}_2=0.2</annotation></semantics></math>y^​1(i)​=y^​2(i)​=0.2比yˆ(i)1=0,yˆ(i)2=0.4y^1(i)=0,y^2(i)=0.4 \hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4<math><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>1</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mn>0</mn><mo separator="true">,</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>2</mn><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4</annotation></semantics></math>y^​1(i)​=0,y^​2(i)​=0.4的损失要小很多，虽然两者都有同样正确的分类预测结果。

- 改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。

- 也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。

- 最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。

- 交叉熵适合衡量两个概率分布的差异。

- 注意： 由于像素值为0到255的整数，所以刚好是uint8所能表示的范围，包括transforms.ToTensor()在内的一些关于图片的函数就默认输入的是uint8型，若不是，可能不会报错但可能得不到想要的结果。所以，如果用像素值(0-255整数)表示图片数据，那么一律将其类型设置成uint8，避免不必要的bug。

- 变量feature对应高和宽均为28像素的图像。由于我们使用了transforms.ToTensor()，所以每个像素的数值为[0.0, 1.0]的32位浮点数。需要注意的是，feature的尺寸是 (C x H x W) 的，而不是 (H x W x C)。第一维是通道数，因为数据集中是灰度图像，所以通道数为1。后面两维分别是图像的高和宽。

- 在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的DataLoader中一个很方便的功能是允许使用多进程来加速数据读取。

- 在下面的函数中，矩阵X的行数是样本数，列数是输出个数。为了表达样本预测各个输出的概率，softmax运算会先通过exp函数对每个元素做指数运算，再对exp矩阵同行元素求和，最后令矩阵每行各元素与该行元素之和相除。这样一来，最终得到的矩阵每行元素和为1且非负。因此，该矩阵每行都是合法的概率分布。softmax运算的输出矩阵中的任意一行元素代表了一个样本在各个输出类别上的预测概率。

- 上一节中，我们介绍了softmax回归使用的交叉熵损失函数。为了得到标签的预测概率，我们可以使用gather函数。

- 通过使用gather函数，我们得到了2个样本的标签的预测概率。与3.4节（softmax回归）数学表述中标签类别离散值从1开始逐一递增不同，在代码中，标签类别的离散值是从0开始逐一递增的。

- 可以使用softmax回归做多类别分类。与训练线性回归相比，你会发现训练softmax回归的步骤和它非常相似：获取并读取数据、定义模型和损失函数并使用优化算法训练模型。事实上，绝大多数深度学习模型的训练都有着类似的步骤。

- 难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。

- 从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络

- 上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。

- 多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。

- 可以通过手动定义模型及其参数来实现简单的多层感知机。

- 计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。

- 在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。基于该独立同分布假设，给定任意一个机器学习模型（含参数），它的训练误差的期望和泛化误差都是一样的

- 机器学习模型应关注降低泛化误差。

- ，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。

- 在机器学习中，通常需要评估若干候选模型的表现并从中选择模型。这一过程称为模型选择（model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。

- 为了得到有效的模型，我们通常要在模型选择上下一番功夫。下面，我们来描述模型选择中经常使用的验证数据集（validation data set）。

- 由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。

- 然而在实际应用中，由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊。从严格意义上讲，除非明确说明，否则本书中实验所使用的测试集应为验证集，实验报告的测试结果（如测试准确率）应为验证结果（如验证准确率）。

- 由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。

- 接下来，我们将探究模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。

- 因为高阶多项式函数模型参数更多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高。因此，高阶多项式函数比低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。

- 。给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。

- 影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。

- 虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。本节介绍应对过拟合问题的常用方法：权重衰减（weight decay）。

- L2​范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。

- 当权重参数均为0时，惩罚项最小。当λλ \lambda<math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>λ较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当λλ \lambda<math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>λ设为0时，惩罚项完全不起作用。上式中L2L2 L_2<math><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math>L2​范数平方∥w∥2∥w∥2 \|\boldsymbol{w}\|^2<math><semantics><mrow><mi mathvariant="normal">∥</mi><mi mathvariant="bold-italic">w</mi><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\|\boldsymbol{w}\|^2</annotation></semantics></math>∥w∥2展开后得到w21+w22w12+w22 w_1^2 + w_2^2<math><semantics><mrow><msubsup><mi>w</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>w</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">w_1^2 + w_2^2</annotation></semantics></math>w12​+w22​。

- 正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。

- 可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。

- optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减 optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr) # 不对偏差参数衰减

- 权重衰减可以通过优化器中的weight_decay超参数来指定。

- 当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。

- 丢弃概率是丢弃法的超参数。

- 丢弃法不改变其输入的期望值

- 由于在训练中隐藏层神经元的丢弃是随机的，即h1,…,h5h1,…,h5 h_1, \ldots, h_5<math><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>h</mi><mn>5</mn></msub></mrow><annotation encoding="application/x-tex">h_1, \ldots, h_5</annotation></semantics></math>h1​,…,h5​都有可能被清零，输出层的计算无法过度依赖h1,…,h5h1,…,h5 h_1, \ldots, h_5<math><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>h</mi><mn>5</mn></msub></mrow><annotation encoding="application/x-tex">h_1, \ldots, h_5</annotation></semantics></math>h1​,…,h5​中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。

- 在PyTorch中，我们只需要在全连接层后添加Dropout层并指定丢弃概率。在训练模型时，Dropout层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时（即model.eval()后），Dropout层并不发挥作用。

- 丢弃法只在训练模型时使用。

- 前面几节里我们使用了小批量随机梯度下降的优化算法来训练模型。在实现中，我们只提供了模型的正向传播（forward propagation）的计算，即对输入计算模型输出，然后通过autograd模块来调用系统自动生成的backward函数计算梯度。

- 我们通常绘制计算图来可视化运算符和变量在计算中的依赖关系。图3.6绘制了本节中样例模型正向传播的计算图，其中左下角是输入，右上角是输出。可以看到，图中箭头方向大多是向右和向上，其中方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系。

- 反向传播指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。

- 一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。

- 另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。

- 因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。

- 深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。

- 当神经网络的层数较多时，模型的数值稳定性容易变差。

- 类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。

- 在神经网络中，通常需要随机初始化模型参数。

- 如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。

- 本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。我们希望读者通过动手操作、仔细观察实验现象、认真分析实验结果并不断调整方法，得到令自己满意的结果。

- 接下来将离散数值转成指示特征。

- 下面的训练函数跟本章中前几节的不同在于使用了Adam优化算法。相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。

- 下面定义比赛用来评价模型的对数均方根误差。

- 我们使用一组未经调优的超参数并计算交叉验证误差。可以改动这些超参数来尽可能减小平均测试误差。

- 注意，这里并没有将Module类命名为Layer（层）或者Model（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如PyTorch提供的Linear类），又可以是一个模型（如这里定义的MLP类），或者是模型的一个部分。

- 下面我们实现一个与Sequential类有相同功能的MySequential类。这或许可以帮助读者更加清晰地理解Sequential类的工作机制。

- ModuleList接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作:

- ModuleList仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现forward功能需要自己实现，所以上面执行net(torch.zeros(1, 784))会报NotImplementedError

- Sequential内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部forward功能已经实现。

- ModuleList的出现只是让网络定义前向传播时更加灵活

- 另外，ModuleList不同于一般的Python的list，加入到ModuleList里面的所有模块的参数会被自动添加到整个网络中

- 和ModuleList一样，ModuleDict实例仅仅是存放了一些模块的字典，并没有定义forward函数需要自己定义。同样，ModuleDict也与Python的Dict有所不同，ModuleDict里的所有模块的参数会被自动添加到整个网络中。

- 在这个网络中，我们通过get_constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用Tensor的函数和Python的控制流，并多次调用相同的层。

- 因为FancyMLP和Sequential类都是Module类的子类，所以我们可以嵌套调用它们。

- 与Sequential不同，ModuleList和ModuleDict并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义forward函数。

- 本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。

- 我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们从nn中导入了init模块，它包含了多种模型初始化方法。

- 对于Sequential实例中含模型参数的层，我们可以通过Module类的parameters()或者named_parameters方法来访问所有参数（以迭代器的形式返回），后者除了返回参数Tensor外还会返回其名字。

- 可见返回的名字自动加上了层数的索引作为前缀。

- 因为这里是单层的所以没有了层数索引的前缀

- 另外返回的param的类型为torch.nn.parameter.Parameter，其实这是Tensor的子类，和Tensor不同的是如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里

- 因为Parameter是Tensor，即Tensor拥有的属性它都有，比如可以根据data来访问参数数值，用grad来访问参数梯度。

- PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考源代码）

- 有时候我们需要的初始化方法并没有在init模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。

- 可以看到这就是一个inplace改变Tensor值的函数，而且这个过程是不记录梯度的。

- 此外，参考2.3.2节，我们还可以通过改变这些参数的data来改写模型参数值同时不会影响梯度:

- 在有些情况下，我们希望在多个层之间共享模型参数。

- 此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的

- 因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的

- 使用PyTorch在定义模型的时候就要指定输入的形状，所以也就不存在这个问题了，所以本节略。

- 深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然PyTorch提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何使用Module来自定义层，从而可以被重复调用。

- 我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。

- 所以在自定义含模型参数的层时，我们应该将参数定义成Parameter，除了像4.2.1节那样直接定义成Parameter类外，还可以使用ParameterList和ParameterDict分别定义参数的列表和字典。

- 而ParameterDict接收一个Parameter实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。

- 我们也可以使用自定义层构造模型。它和PyTorch的其他层在使用上很类似。

- 可以通过Module类自定义神经网络中的层，从而可以被重复调用。

- save使用Python的pickle实用程序将对象进行序列化，然后将序列化的对象保存到disk，使用save可以保存各种对象,包括模型、张量和字典等。而load使用pickle unpickle工具将pickle的对象文件反序列化为内存。

- PyTorch可以指定用来存储和计算的设备，如使用内存的CPU或者使用显存的GPU。默认情况下，PyTorch会将数据创建在内存，然后利用CPU来计算。

- PyTorch要求计算的所有输入数据都在内存或同一块显卡的显存上。

- 如果对在GPU上的数据进行运算，那么结果还是存放在GPU上。

- 需要注意的是，存储在不同位置中的数据是不可以直接进行计算的。即存放在CPU上的数据不可以直接与存放在GPU上的数据进行运算，位于不同GPU上的数据也是不能直接进行计算的。

- 同Tensor类似，PyTorch模型也可以通过.cuda转换到GPU上。我们可以通过检查模型的参数的device属性来查看存放模型的设备。

- 卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。

- 虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。

- 在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。

- 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。

- 最后我们来看一个例子，它使用物体边缘检测中的输入数据X和输出数据Y来学习我们构造的核数组K。我们首先构造一个卷积层，其卷积核将被初始化成随机数组。接下来在每一次迭代中，我们使用平方误差来比较Y和卷积层的输出，然后计算梯度来更新权重。

- 实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。

- 那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。

- 为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。

- 二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素xx x<math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做xx x<math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x的感受野（receptive field）。

- 二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。

- 所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。

- 在很多情况下，我们会设置ph=kh−1ph=kh−1 p_h=k_h-1<math><semantics><mrow><msub><mi>p</mi><mi>h</mi></msub><mo>=</mo><msub><mi>k</mi><mi>h</mi></msub><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p_h=k_h-1</annotation></semantics></math>ph​=kh​−1和pw=kw−1pw=kw−1 p_w=k_w-1<math><semantics><mrow><msub><mi>p</mi><mi>w</mi></msub><mo>=</mo><msub><mi>k</mi><mi>w</mi></msub><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p_w=k_w-1</annotation></semantics></math>pw​=kw​−1来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。

- 卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。

- 填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。

- 步幅可以减小输出的高和宽

- 本节我们将介绍含多个输入通道或多个输出通道的卷积核。

- 当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。

- 当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。

- 如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为ci×kh×kwci×kh×kw c_i\times k_h\times k_w<math><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>×</mo><msub><mi>k</mi><mi>h</mi></msub><mo>×</mo><msub><mi>k</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">c_i\times k_h\times k_w</annotation></semantics></math>ci​×kh​×kw​的核数组。

- 使用多通道可以拓展卷积层的模型参数。

- 1×1卷积层通常用来调整网络层之间的通道数，并控制模型复杂度。

- 但实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后面的模式识别造成不便。

- 在本节中我们介绍池化（pooling）层，它的提出是为了缓解卷积层对位置的过度敏感性。

- 不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。

- 在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。

- 池化层的一个主要作用是缓解卷积层对位置的过度敏感性。

- 池化层的输出通道数跟输入通道数相同。

- 卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet [1]。

- 在3.9节（多层感知机的从零开始实现）里我们构造了一个含单隐藏层的多层感知机模型来对Fashion-MNIST数据集中的图像进行分类。

- 然而，这种分类方法有一定的局限性。

- 对于大尺寸的输入图像，使用全连接层容易造成模型过大。

- 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。

- 这带来过复杂的模型和过高的存储开销。

- 一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。

- LeNet分为卷积层块和全连接层块两个部分。

- 卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。

- 由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。

- 卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。

- 第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。

- 第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。

   - Annotation: 各卷积层的参数尺寸类似，以确保每个卷积层的计算负担和学习能力大致相当，有助于网络的稳定训练。- 可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。

- # 如果没指定device就使用net的device

- LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。

- 在LeNet提出后的将近20年里，神经网络一度被其他机器学习方法超越，如支持向量机。虽然LeNet可以在早期的小数据集上取得好的成绩，但是在更大的真实数据集上的表现并不尽如人意。

- 另一方面，当年研究者还没有大量深入研究参数初始化和非凸优化算法等诸多领域

- 一方面，神经网络计算复杂。

- 我们在上一节看到，神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end）的方法节省了很多中间步骤。

- 既然特征如此重要，它该如何表示呢？

- 我们已经提到，在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。

- 然而，另一些研究者则持异议。他们认为特征本身也应该由学习得来。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。

- 需要强调的是，输入的逐级表示由多层模型中的参数决定，而这些参数都是学出来的。

- 包含许多特征的深度模型需要大量的有标签的数据才能表现得比其他经典方法更好。

- 深度学习对计算资源要求很高。早期的硬件计算能力有限，这使训练较复杂的神经网络变得很困难。

- 它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。

- AlexNet与LeNet的设计理念非常相似，但也有显著的区别。

- AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。

- 虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。

- 第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。

- 而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。

- 紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。

- 第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数

- 一方面，ReLU激活函数的计算更简单

- 另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。

- 因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。

- 第三，AlexNet通过丢弃法（参见3.13节）来控制全连接层的模型复杂度。

- 第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

- 虽然论文中AlexNet使用ImageNet数据集，但因为ImageNet数据集训练时间较长，我们仍用前面的Fashion-MNIST数据集来演示AlexNet。

- 相对于LeNet，由于图片尺寸变大了而且模型变大了，所以需要更大的显存，也需要更长的训练时间了。

- AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。

- VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。

- 对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。

- 这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。

- 现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输入输出通道分别是1（因为下面要使用的Fashion-MNIST数据的通道数为1）和64，之后每次对输出通道数翻倍，直到变为512。

- VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。

- 该变量指定了每个VGG块里卷积层个数和输入输出通道数。

- 可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。

- 因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。

- 本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。

- 前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。

- 如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维

- 1×1卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×11×1 1\times 1<math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math>1×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。

- NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1×11×1 1\times 1<math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math>1×1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。

- 每个NiN块后接一个步幅为2、窗口形状为3×33×3 3\times 3<math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math>3×3的最大池化层。

- 除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。

- NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。

- 全局平均池化层即窗口形状等于输入空间维形状的平均池化层

- NiN重复使用由卷积层和代替全连接层的1×11×1 1\times 1<math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math>1×1卷积层构成的NiN块来构建深层网络。

- NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。

- 它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。

- 由图5.8可以看出，Inception块里有4条并行的线路。

- 4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。

- GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的3×33×3 3\times 3<math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math>3×3最大池化层来减小输出高宽。

- 需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。

- GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。本节里我们将输入的高和宽从224降到96来简化计算。

- Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用1×11×1 1\times 1<math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math>1×1卷积层减少通道数从而降低模型复杂度。

- GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。

- 本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易 [1]。

- 标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。

- 通常来说，数据标准化预处理对于浅层模型就足够有效了。

- 对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。

- 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。

- 对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。

- 通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。

- 如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。

- 值得注意的是，可学习的拉伸和偏移参数保留了不对xˆ(i)x^(i) \hat{\boldsymbol{x}}^{(i)}<math><semantics><mrow><msup><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{x}}^{(i)}</annotation></semantics></math>x^(i)做批量归一化的可能：此时只需学出γ=σ2B+ϵ−−−−−−√γ=σB2+ϵ \boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}<math><semantics><mrow><mi mathvariant="bold-italic">γ</mi><mo>=</mo><msqrt><mrow><msubsup><mi mathvariant="bold-italic">σ</mi><mi mathvariant="script">B</mi><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}</annotation></semantics></math>γ=σB2​+ϵ​和β=μBβ=μB \boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}<math><semantics><mrow><mi mathvariant="bold-italic">β</mi><mo>=</mo><msub><mi mathvariant="bold-italic">μ</mi><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}</annotation></semantics></math>β=μB​。

- 至此，我们得到了x(i)x(i) \boldsymbol{x}^{(i)}<math><semantics><mrow><msup><mi mathvariant="bold-italic">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{x}^{(i)}</annotation></semantics></math>x(i)的批量归一化的输出y(i)y(i) \boldsymbol{y}^{(i)}<math><semantics><mrow><msup><mi mathvariant="bold-italic">y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{y}^{(i)}</annotation></semantics></math>y(i)。

- 在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 γγ \boldsymbol{\gamma}<math><semantics><mrow><mi mathvariant="bold-italic">γ</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math>γ 和偏移（shift）参数 ββ \boldsymbol{\beta}<math><semantics><mrow><mi mathvariant="bold-italic">β</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>β。

- 对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。

- 和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。

- 使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。

- 将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。

- 对全连接层和卷积层做批量归一化的方法稍有不同。

- 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。

- 如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。

- 对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中m×p×qm×p×q m \times p \times q<math><semantics><mrow><mi>m</mi><mo>×</mo><mi>p</mi><mo>×</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">m \times p \times q</annotation></semantics></math>m×p×q个元素的均值和方差。

- 由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet） [1]。

- 在残差块中，输入可通过跨层的数据线路更快地向前传播。

- 残差映射在实际中往往更容易优化。

- 左图虚线框中的部分需要直接拟合出该映射f(x)f(x) f(\boldsymbol{x})<math><semantics><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold-italic">x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(\boldsymbol{x})</annotation></semantics></math>f(x)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射f(x)−xf(x)−x f(\boldsymbol{x})-\boldsymbol{x}<math><semantics><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold-italic">x</mi><mo>)</mo><mo>−</mo><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">f(\boldsymbol{x})-\boldsymbol{x}</annotation></semantics></math>f(x)−x。

- 实际中，当理想映射f(x)f(x) f(\boldsymbol{x})<math><semantics><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold-italic">x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(\boldsymbol{x})</annotation></semantics></math>f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。

