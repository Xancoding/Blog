---
doc_type: hypothesis-highlights
url: 'https://zhuanlan.zhihu.com/p/112295277'
---
# 图表示学习极简教程
## Metadata
- Author: [zhuanlan.zhihu.com]()
- Title: 图表示学习极简教程
- Reference: https://zhuanlan.zhihu.com/p/112295277
- Category: #source/article🗞
- Tags:
## Highlights
- 图表示学习主要分为基于图结构的表示学习和基于图特征的表示学习。

- 图表示学习的task就是用 nnn 个向量表示图上的 nnn 个结点，这样我们就可以将一个难以表达的拓扑结构转化为可以丢进炼丹炉的vector啦。

- ​ 在我们的图表示学习中，我们希望Embedding出来的向量在图上“接近”时在向量空间也“接近”。对于第2个“接近”，就是欧式空间两个向量的距离。对于第一个“接近”，可以有很多的解释

- 结构性相对于异质性而言。异质性关注的是结点的邻接关系；结构性将两个结构上相似的结点定义为“临近”。比方说，某两个点是各自小组的中心，这样的两个节点就具有结构性。

- DeepWalk的方法采用了Random walk的思想进行结点采样

- 我们首先根据用户的行为构建出一个图网络；随后通过Random walk随机采样的方式构建出结点序列(例如：一开始在A结点，A->B，B又跳到了它的邻居结点E，最后到F，得到"A->B->E->F"序列)；对于序列的问题就是NLP中的语言模型，因为我们的句子就是单词构成的序列。接下来我们的问题就变成Word2vec(词用向量表示)的问题，我们可以采用Skip-gram的模型来得到最终的结点向量。可以说这种想法确实是十分精妙，将图结构转化为序列问题确实是非常创新的出发点。

- ​ 在这里，结点走向其邻居结点的概率是均等的。

- ​ 之前所述的Random Walk方法中，一个结点向邻居结点游走的概率是相等的。这种等概率的游走操作似乎是比较naive的，对此，Node2vec的提出就是对结点间游走概率的定义。

- 我们发现，当 ppp 比较小的时候，结点间的跳转类似于BFS，结点间的“接近”就可以理解为结点在邻接关系上“接近”；当 qqq 比较小的时候，结点间的跳转类似于DFS，节点间的“接近”就可以视作是结构上相似。具体可借助图5理解。

- ​ 针对异构图而言，其结点的属性不同，采样的方式也与传统的图网络不同，需要按照定义的Meta-Path规则进行采样。

- ​ 在基于图特征的表示学习中，由于加入了结点的特征矩阵 XXX (姓名、年龄、身高等这样的特征)，需要同基于图结构的表示学习区别开来。这一类的模型通常被叫做“图神经网络”。

- 公式(3.1-1)、公式(3.1-2)和上面的一堆推导可以说是殊途同归。其核心的意思都表示的是“邻居结点的聚合”。

- ​ 从直观层面，GCN的含义就是“聚合”的含义。

- 从图信号角度对GCN进行分析，又会是一番新的境界。某在另一篇文章：图卷积网络(GCN)的谱分析中对如何从频域的视角理解GCN以及GCN的过平滑问题做了较为深入的探讨

- ​ GCN提出之后，同年的NIPS上GraphSAGE也横空出世，其主要的特点是固定的采样倍率和不同的聚合方式。

- 对于邻居节点的聚合，GraphSAGE在一个epoch中，作者并未将某个结点所有的邻居结点进行聚合，而是设置一个定值 SkS_kS_k ，在第 kkk 层选择邻居的时候只从中选最多只选择 SkS_kS_k 个邻居结点进行聚合，计算的复杂度大致在 O(∏k=1KSk)\mathcal{O}(\prod_{k=1}^KS_k)\mathcal{O}(\prod_{k=1}^KS_k) 这个水平。这一改进对GNN的落地有着重要意义

- 与平均聚合相比，LSTM聚合具有更大的表达能力。

- 在图学习中，Graph Attention Network就是引入了注意力机制的GNN。

- ​ 此外，作者在提出GAT时也提出了多头注意力机制的方法。

