---
doc_type: hypothesis-highlights
url: 'https://www.cnblogs.com/nickchen121/p/11686715.html'
---
# A-02 梯度下降法 - 水论文的程序猿 - 博客园
## Metadata
- Author: [cnblogs.com]()
- Title: A-02 梯度下降法 - 水论文的程序猿 - 博客园
- Reference: https://www.cnblogs.com/nickchen121/p/11686715.html
- Category: #source/article🗞
- Tags:
## Highlights
- 在求解机器学习算法模型参数的时候，梯度下降法（gradient descent）和最小二乘法（least squares）是最经常使用的方法，由于梯度下降法衍生出的分支较多，所以在这里对梯度下降法单独做一个总结。


- Tags:

- 在机器学习算法中，如果需要最小化目标函数时，则可以通过梯度下降法一步一步的迭代求解，得到最小化的目标函数和模型参数值；如果要最大化目标函数时，则可以通过梯度上升法迭代求解。

- 梯度下降算法和梯度上升法之间也可以互相转化，可以通过梯度下降迭代最小化目标函数，同时也可以反向梯度上升迭代最小化目标函数；同理也可以反向梯度下降迭代最大化目标函数。

- 梯度下降不一定能够找到全局的最优解，有可能会找到局部最优解

- 但是如果代价函数为凸函数的时候梯度下降得到的则一定是全局最优解


- Tags:

- 三种不同形式的梯度下降法

- 三种不同形式的梯度下降法步骤都是相同的，只是在更新参数的时候选择的样本数量不同


- Tags:

- 批量梯度下降法（batch gradient descent）是最常用的做法，它使用所有的样本更新参数

