---
doc_type: hypothesis-highlights
url: 'https://zhuanlan.zhihu.com/p/41583326'
---
# 深度学习、机器学习与NLP的前世今生
## Metadata
- Author: [zhuanlan.zhihu.com]()
- Title: 深度学习、机器学习与NLP的前世今生
- Reference: https://zhuanlan.zhihu.com/p/41583326
- Category: #source/article🗞
- Tags:
## Highlights
- 在深度学习在NLP领域火起来之前，最有代表性的一个研究、对每个人影响最大的工作就是Word2Vec，把一个字、一个词变成向量来表示，这是对我们影响非常大的工作。

- 这件事情的好处是什么？在之前我们以词为单位，一个词的表示方式几乎都是one hot， one hot序列有一个致命的缺点，你不能计算相似度，所有人算出来都是“0”，都是一样的，距离也都是一样的，因此它不能很好的表示词之间的关系。

- 第一，这个词如果有1万维的话，1万维本来存储它就是一个非常稀疏的矩阵、而且很浪费，我们就可以把它变得更小，因为我们的Word2Vec里面一般的向量都在 512以内。这个维度的向量相对1万维来说已经是比较低维的空间，它里面存的是各种的浮点数，这个浮点数看起来这三个向量好像每个都不一样，但是实际去计算，发现这三个向量之间的相似度非常高，一个是相似度可以判断它的相似性，另外是判断它们的距离。


- Tags:

- 威海、潍坊、枣庄这几个城市在空间上离得非常近，它们的数值也非常近。它对于我们实际工作的好处是增强了我们的泛化能力，这是一个很难做的事情。

- 第一，有更好的带语义的表示。

- 第二，有了这样的表示之后可以做语义的计算，包括山东-威海约等于广东-佛山，两个向量之间是约等于的，语义的东西不太好解释，但是人知道这是怎么回事，语义相近就是Word2Vec最大的帮助。

- 为什么叫深度学习？我们这只是一层，它在CNN里面尤其图像识别网络，大家都听过“大力出奇迹”，网络越深效果越好，因为它经过一层一层的学习，可以把每一层的特征进行浓缩。简单的像素没有任何的表义能力，到第一层浓缩之后它有一些点线的能力，再往上浓缩可能就有弧线的能力，再往上浓缩它越来越复杂，可以做到把一个像素这个没有意义的东西变成有意义的东西，可以它可以看成是一层层的过滤，选出最好的特征结果，这是卷积的原理。卷积不仅仅在图像里，在文本里用得也非常好。


- Tags:

- 3、many to one。输入的是一个序列，文字等都是这样一个序列，这个序列输出之后做文本分类、情感分析，它最终都给出来这样一个结果，它们都属于“多到一”的过程。

- 2、one to many。图像描述。

- 一个图像进来了，它告诉我图像上有一个狗、一个猫站在车旁边，这就是一个图像描述的过程，它可以把图像变成很多输出，这就是one to many的问题。

- 、one to one。图像分类

- 传统的机器学习，需要构造特征，不同领域定制化程度很高，这个模型A领域用了，B领域几乎要从头再做一遍，没有办法把其他的特征迁移过来很好的使用。某些领域效果很好，某些领域另外一个算法很好，传统机器学习把各种各样的方式做以融合来提升效果。


- Tags:

- 深度学习则可实现端到端，无需大量特征工程。框架的通用性也很好，能满足多领域的需求，并且可以使用费监督语料训练字词向量提升效果。但是为什么有人吐槽Deep learning？因为调参很麻烦，有时改了一下参数好很多，改了一个参数效果又下降了，有的算法能够对此有一定的解释，但不像传统机器学习能够解释得那么好。这两大帮派不能说完全谁战胜了谁，是相融相生的。


- Tags:

- 生成式摘要是很难的一个东西，它的训练集标注比我们标分词、标分类难得多，要有一篇文章，人得写出摘要，整理出好多这样的摘要，因为每个人写得不一样，包括评测的方式BLUE等，所以做摘要比较难。但是我们平时可以基于生成式文本的其他小应用。

- 举个简单的例子，大家爬过一些新闻的网站，那么长的正文一般正文第一段把事情都说清楚了，然后有一个新闻的标题，我们可以用第一段作为输入，标题作为输出，做这样一个简单的通过新闻第一段可以写出新闻标题的功能，其实跟生成摘要的思想是一样的。唯一的差别是它加了注意力的机制，会发现它关注输出的哪些词对语义表达最有用，它会关注有用的信息，解码的时候就可以得到各种各样的序列、各种各样的值，用beam search找到最好的结果。

